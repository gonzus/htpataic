<!DOCTYPE html>
<html>
<head>
<title>17. Test automation</title>
<link rel="stylesheet" href="htpataic.css" type="text/css" />
</head>
<body>
<table class="contents"><tr><td>
#contents
</td></tr></table>

<h1>How to program a text adventure in C</h1>
<p>
by Ruud Helderman
&lt;<a href="mailto:r.helderman@hccnet.nl">r.helderman@hccnet.nl</a>&gt;
</p>
<p>
Licensed under
<a href="https://github.com/helderman/htpataic/blob/master/LICENSE">MIT License</a>
</p>
<h2>17. Test automation</h2>
<p class="intro">
Testing is important in any software development project.
Games are no exception.
Dutch games developer
<a href="https://en.wikipedia.org/wiki/John_Vanderaart">John Vanderaart</a>
made it very clear in
<a href="http://www.nostalgia8.org/download/commodore/interview/%5Binterview%5Djohn.html">an interview in 2007</a>:
&ldquo;test, test, test.&rdquo;
</p>
<p>
You could test everything
<a href="https://en.wikipedia.org/wiki/Manual_testing">manually</a>;
play your own game to see if everything still works.
Over and over again.
Every change you make, no matter how small, should be tested.
This quickly becomes tiresome.
</p>
<p>
Fortunately, text adventures are very suitable for
<a href="https://en.wikipedia.org/wiki/Test_automation">test automation</a>.
In its simplest form, this works a follows.
</p>
<ol type="1">
<li>Test the game manually once,
while letting the program <b>log every command</b> you enter.
</li>
<li>Run the game again, but this time, use the log file as input.
<b>Capture the output</b> in a file.
</li>
<li>Whenever you want to test the program,
just repeat step 2, and <b>compare the output</b> from both sessions.
Any differences that cannot be justified, should be considered bugs.
</li>
</ol>
<p>
Step 1 is easy; we implemented this in the previous chapter.
All we have to do is run the program with a filename as argument.
</p>
<table><tr>
<td class="snippet">./lilcave testscript.txt
</td>
</tr></table>
<p>
Now start testing the game manually.
Try as many scenarios as you can.
Just a quick walk-through is not enough;
you really need to test the dead ends and pitfalls.
Try to get a good
<a href="https://en.wikipedia.org/wiki/Code_coverage">code coverage</a>;
use tooling like
<a href="https://en.wikipedia.org/wiki/Gcov">gcov</a>
where possible.
When you are done, exit the program.
</p>
<p>
Keep the resulting log file in a safe place; it is your
<a href="https://en.wikipedia.org/wiki/Test_script">test script</a>.
</p>
<p>
There is one more thing to do here:
add the command &lsquo;quit&rsquo; to the end of the file.
</p>
<table><tr>
<td class="snippet">echo quit &gt;&gt; testscript.txt
</td>
</tr></table>
<p>
We are a bit cheating here;
normally a log file never contains the command &lsquo;quit&rsquo;,
because that would make it impossible for the player to continue the game.
But in an automated test, we really <i>want</i> the program to quit
once the test script has been finished.
</p>
<table class="code"><tr>
<th>testscript.txt</th>
</tr><tr>
<td>
#new testscript.txt
</td>
</tr></table>
<p>
This test script is full of weird (combinations of) commands.
I did my best to let the program give as many different responses as possible.
The result is a code coverage of more than 90%, which is pretty good.
Please note that this does <i>not</i> imply
that we have tested 90% of all possible verb/noun combinations.
Code coverage says little to nothing about the number of <i>objects</i>
that we interacted with, due to the data-driven nature of our program.
</p>
<p>
In step 2, we just call the program again, with the same filename as argument.
Effectively, this repeats the entire session of step 1,
without any user intervention.
The difference is, this time we
<a href="https://en.wikipedia.org/wiki/Redirection_%28computing%29">redirect</a>
the output of the program to a file.
</p>
<table><tr>
<td class="snippet">./lilcave testscript.txt &gt; baseline.txt
</td>
</tr></table>
<p>
This file is a transcript of the entire test session.
Keep it in a safe place as well; it is our
<a href="https://en.wikipedia.org/wiki/Baseline_%28configuration_management%29">baseline</a>.
</p>
<table class="code"><tr>
<th>baseline.txt</th>
</tr><tr>
<td>
#new baseline.txt
</td>
</tr></table>
<p>
We now have two files, the <b>test script</b> and the <b>baseline</b>,
that we will be using to test every change we make to the program.
Testing is easy:
</p>
<table><tr>
<td class="snippet">./lilcave testscript.txt &gt; transcript.txt
diff baseline.txt transcript.txt
</td>
</tr></table>
<p>
Above, I used <i>diff</i> to compare
the actual output of the game (<i>transcript.txt</i>)
with the output that was last considered correct (<i>baseline.txt</i>).
Please feel free to use your favorite
<a href="https://en.wikipedia.org/wiki/Diff_utility">diff utility</a>
instead.
</p>
<p>
Now there are three possible outcomes.
</p>
<ol type="1">
<li>There are no differences.
Apparently, your changes did not result in a
<a href="https://en.wikipedia.org/wiki/Software_regression">regression</a>.
</li>
<li>All differences were intentional.
For example, after fixing a typo,
the game&rsquo;s output should only change for the better.
</li>
<li>There was an unintentional difference.
This is a potential
<a href="https://en.wikipedia.org/wiki/Software_bug">bug</a>;
it should be analyzed and if necessary, fixed.
</li>
</ol>
<p>
Please note that case 2 is a reason for your baseline to be updated;
otherwise your intentional differences will keep piling up,
making it increasingly hard to spot the unintentional differences.
If there are no unintended changes,
then updating the baseline is straightforward:
</p>
<table><tr>
<td class="snippet">cp transcript.txt baseline.txt
</td>
</tr></table>
<p>
Obviously, changing your test script
(e.g. because you introduced new code or new objects
not covered by the original test script)
will always break the test.
This warrants a new manual test, resulting in a new baseline.
Of course, you can use (part of) the existing test script as a starting point;
rarely will it be necessary to start all over.
</p>
<p>
Consider including test automation in your build process.
Here is an example for
<a href="https://en.wikipedia.org/wiki/Make_%28software%29">make</a>:
</p>
<table><tr>
<td class="snippet">success.txt: lilcave testscript.txt baseline.txt
	./lilcave testscript.txt &gt; transcript.txt
	cmp baseline.txt transcript.txt
	mv -f transcript.txt success.txt
</td>
</tr></table>
<p>
Here, <i>make</i> will only execute the final move
(<a href="https://en.wikipedia.org/wiki/Mv">mv</a>)
after a successful comparison
(<a href="https://en.wikipedia.org/wiki/Cmp_(Unix)">cmp</a>).
That way, an invalid transcript can never be mistaken for a successful update,
and <i>make</i> will always re-run a test that failed earlier.
</p>
<h3>Performance testing</h3>
<p>
Performance is hardly ever an issue in text adventures,
for a number of reasons.
</p>
<ul>
<li>Most text adventures do not take place in real time.
To win the game, the player needs to be clever rather than fast.
Most of the time, the program will be waiting for a reply from the user,
not the other way around.
Even if combat is involved (see chapter 20), it will be turn-based.
A response time of 100 milliseconds or more should be totally acceptable.
</li>
<li>Most text adventures are
<a href="https://en.wikipedia.org/wiki/Single-player_video_game">single-player</a>
games.
There are no
<a href="https://en.wikipedia.org/wiki/Scalability">scalability</a>
issues to be solved.
</li>
<li>And considering the code we have been writing so far:
C compiles to
<a href="https://en.wikipedia.org/wiki/Machine_code">native code</a>,
which is fast by nature.
Especially when there are no performance bottlenecks like
network traffic and complex
<a href="https://en.wikipedia.org/wiki/Memory_management">memory management</a>
(<a href="https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)">garbage collections</a>).
</li>
</ul>
<p>
Still, it is always good to take performance seriously.
Looking back at our code so far, you may have noticed some parts
that are potentially suboptimal.
</p>
<ul>
<li>Each time the player enters a command,
the parser loops through all possible command patterns,
trying to match each one to the user input.
</li>
<li>Most of these patterns contain 'nonterminals';
blanks that need to be filled in by finding an object
that matches the noun entered by the player.
The parser will scan through the full set of objects,
then for each object, scan through all its tags.
That's a loop nested inside a loop.
</li>
<li>Commands like <i>look</i> and <i>inventory</i> use a similar loop
to scan through the complete set of objects,
in search of the ones relevant at the current location.
</li>
</ul>
<p>
In a big adventure, with many locations, items, actors,
and with a comprehensive vocabulary,
these loops will have to traverse some pretty long arrays.
To make matters worse,
the 'log and roll forward' technique proposed in the previous chapter
means the player will have to wait for hundreds, maybe thousands of commands
to be parsed and executed before being able to resume a game saved earlier.
How long is that going to take?
</p>
<p>
Many programmers will not wait for the bomb to burst,
and immediately start replacing the loops by more advanced techniques,
for example
<a href="https://en.wikipedia.org/wiki/Hash_table">hash tables</a>.
This could make your code more efficient, but possibly also more complex.
I strongly recommend every developer to take some time to study the
<a href="http://wiki.c2.com/?RulesOfOptimization">rules of optimization</a>
first:
</p>
<ul>
<li>Rule #1: Don't.</li>
<li>Rule #2: Don't... <i>yet</i>.</li>
<li>Rule #3: Profile before optimizing.</li>
</ul>
<p>
The simplest way to profile your code,
is to run an automated test while keeping track of time.
In most operating systems, you can use
<a href="https://en.wikipedia.org/wiki/Time_(Unix)">time</a>
for that.
If you want a more detailed insight, then you can use
<a href="https://en.wikipedia.org/wiki/Gprof">gprof</a>;
it can tell you exactly what functions eat up the majority of CPU time.
</p>
<p>
I leave it up to you to decide whether or not to make
<a href="https://en.wikipedia.org/wiki/Software_performance_testing">performance testing</a>
an integral part of your test automation.
</p>
<hr />
<table class="download"><tr><td>
#zip
</td></tr></table>
<p>
Next chapter: <a href="htpataic18.html">18. Abbreviations</a>
</p>
</body>
</html>
